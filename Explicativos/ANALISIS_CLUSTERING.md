# üìä An√°lisis de la Fase de Clustering - Pipeline Data Science

## üéØ Objetivo General
La fase de clustering agrupa los datos de ventas en clusters (grupos) similares sin necesidad de etiquetas previas. Esto permite descubrir patrones ocultos en los datos y segmentar productos/comunas por comportamiento de ventas.

---

## üì• Entrada del Pipeline de Clustering

### Dataset de Entrada: `ventas_preprocesadas`
- **Tipo**: `pandas.ParquetDataset`
- **Ubicaci√≥n**: `data/03_primary/ventas_preprocesadas.parquet`
- **Origen**: Output del pipeline de `data_processing`
- **Columnas**: FECHA, PRODUCTO, COMUNA, CANTIDAD, MES, PRODUCTO_ID, COMUNA_ID, VENTA_MES_ANTERIOR, AUMENTA, VENTA_CLASE

---

## üîÑ Flujo del Pipeline de Clustering (15 Nodos)

### **NODO 1: `prepare_clustering_data_node`**
**Funci√≥n**: `prepare_clustering_data()`

#### ¬øQu√© hace?
- **Prepara los datos** para algoritmos de clustering
- **Selecciona features num√©ricas** autom√°ticamente
- **Muestrea los datos** si son muy grandes (m√°ximo 10,000 muestras por defecto)

#### Transformaciones espec√≠ficas:
1. **Muestreo** (si `len(data) > max_samples`):
   - Toma una muestra aleatoria de `max_samples` filas
   - Usa `random_state=42` para reproducibilidad
   - **Prop√≥sito**: Reducir tiempo de c√≥mputo en datasets grandes

2. **Selecci√≥n de features**:
   - Si `feature_columns` es `None`, selecciona autom√°ticamente todas las columnas num√©ricas
   - Filtra solo columnas de tipo num√©rico (`np.number`)
   - **Resultado**: DataFrame solo con features num√©ricas

#### Output: `X_clustering`
- **Tipo**: `pandas.ParquetDataset`
- **Ubicaci√≥n**: `data/04_feature/X_clustering.parquet`
- **Estado**: DataFrame con solo features num√©ricas, listo para clustering

---

### **NODO 2: `scale_features_node`**
**Funci√≥n**: `scale_features()`

#### ¬øQu√© hace?
- **Estandariza las features** usando `StandardScaler`
- **Normaliza** todas las variables a media 0 y desviaci√≥n est√°ndar 1
- **Guarda el scaler** para poder transformar nuevos datos

#### Transformaciones espec√≠ficas:
1. **Estandarizaci√≥n**:
   - Aplica `StandardScaler.fit_transform()` a todas las features
   - F√≥rmula: `(x - media) / desviaci√≥n_est√°ndar`
   - **Prop√≥sito**: Todas las features tienen la misma escala (importante para clustering)

2. **Preservaci√≥n de estructura**:
   - Mantiene nombres de columnas originales
   - Mantiene √≠ndices originales

#### Outputs:
- **`X_clustering_scaled`**: Features estandarizadas (ParquetDataset)
- **`scaler_clustering`**: Objeto StandardScaler guardado (PickleDataset)
  - **Ubicaci√≥n**: `data/06_models/scaler_clustering.pickle`
  - **Uso**: Para estandarizar nuevos datos en producci√≥n

---

### **NODOS 3-5: K-Means Clustering**

#### **NODO 3: `train_kmeans_node`**
**Funci√≥n**: `train_kmeans()`

##### ¬øQu√© hace?
- **Entrena un modelo K-Means** con 3 clusters por defecto
- **Agrupa los datos** en k grupos bas√°ndose en distancia euclidiana

##### Par√°metros:
- `n_clusters`: 3 (por defecto)
- `random_state`: 42 (reproducibilidad)
- `n_init`: 10 (intentos de inicializaci√≥n)

##### Outputs:
- **`modelo_kmeans`**: Modelo KMeans entrenado (PickleDataset)
- **`labels_kmeans`**: Etiquetas de cluster asignadas a cada muestra (PickleDataset)

#### **NODO 4: `evaluate_kmeans_node`**
**Funci√≥n**: `evaluate_clustering()`

##### ¬øQu√© hace?
- **Eval√∫a la calidad** del clustering K-Means
- **Calcula m√©tricas** de evaluaci√≥n

##### M√©tricas calculadas:
1. **Silhouette Score**: Mide qu√© tan bien separados est√°n los clusters
   - Rango: -1 a 1 (m√°s alto = mejor)
   - Mide cohesi√≥n interna y separaci√≥n entre clusters

2. **Davies-Bouldin Score**: Mide la separaci√≥n entre clusters
   - Rango: 0 a ‚àû (m√°s bajo = mejor)
   - Considera la distancia entre clusters y su tama√±o

3. **Calinski-Harabasz Score**: Ratio de varianza entre clusters vs dentro de clusters
   - Rango: 0 a ‚àû (m√°s alto = mejor)
   - Tambi√©n conocido como "Variance Ratio Criterion"

4. **Informaci√≥n adicional**:
   - `n_clusters`: N√∫mero de clusters encontrados
   - `n_noise`: Puntos de ruido (solo para DBSCAN)

##### Output: `metricas_kmeans`
- **Tipo**: `pickle.PickleDataset`
- **Ubicaci√≥n**: `data/08_reporting/metricas_kmeans.pickle`
- **Contenido**: Diccionario con todas las m√©tricas

#### **NODO 5: `add_clusters_kmeans_node`**
**Funci√≥n**: `add_cluster_labels_to_data()`

##### ¬øQu√© hace?
- **Agrega las etiquetas de cluster** al DataFrame original
- **Crea una nueva columna** con el n√∫mero de cluster asignado

##### Transformaciones:
- Agrega columna `cluster` (o nombre personalizado) con los labels
- Asegura que los tama√±os coincidan (trunca si es necesario)

##### Output: `datos_con_clusters_kmeans`
- **Tipo**: `pandas.ParquetDataset`
- **Ubicaci√≥n**: `data/05_model_input/datos_con_clusters_kmeans.parquet`
- **Estado**: Datos originales + columna de cluster K-Means

---

### **NODOS 6-8: DBSCAN Clustering**

#### **NODO 6: `train_dbscan_node`**
**Funci√≥n**: `train_dbscan()`

##### ¬øQu√© hace?
- **Entrena un modelo DBSCAN** (Density-Based Spatial Clustering)
- **Encuentra clusters** bas√°ndose en densidad de puntos
- **Identifica puntos de ruido** (outliers)

##### Par√°metros:
- `eps`: 0.5 (distancia m√°xima entre puntos del mismo cluster)
- `min_samples`: 5 (m√≠nimo de puntos para formar un cluster)

##### Caracter√≠sticas especiales:
- **No requiere especificar n√∫mero de clusters** (lo encuentra autom√°ticamente)
- **Puede identificar puntos de ruido** (label = -1)
- **√ötil para encontrar outliers** y clusters de forma irregular

##### Outputs:
- **`modelo_dbscan`**: Modelo DBSCAN entrenado (PickleDataset)
- **`labels_dbscan`**: Etiquetas de cluster (incluye -1 para ruido) (PickleDataset)

#### **NODO 7: `evaluate_dbscan_node`**
**Funci√≥n**: `evaluate_clustering()`

##### ¬øQu√© hace?
- **Eval√∫a DBSCAN** con las mismas m√©tricas que K-Means
- **Filtra puntos de ruido** antes de calcular m√©tricas
- **Reporta n√∫mero de clusters y puntos de ruido**

##### Output: `metricas_dbscan`
- **Tipo**: `pickle.PickleDataset`
- **Ubicaci√≥n**: `data/08_reporting/metricas_dbscan.pickle`

#### **NODO 8: `add_clusters_dbscan_node`**
**Funci√≥n**: `add_cluster_labels_to_data()`

##### Output: `datos_con_clusters_dbscan`
- **Tipo**: `pandas.ParquetDataset`
- **Ubicaci√≥n**: `data/05_model_input/datos_con_clusters_dbscan.parquet`
- **Nota**: Puede contener puntos con label -1 (ruido/outliers)

---

### **NODOS 9-11: Agglomerative Clustering**

#### **NODO 9: `train_agglomerative_node`**
**Funci√≥n**: `train_agglomerative_with_fallback()`

##### ¬øQu√© hace?
- **Entrena Agglomerative Clustering** (clustering jer√°rquico)
- **Usa PCA** para reducir dimensiones y optimizar memoria
- **Tiene fallback** si n_clusters es inv√°lido

##### Caracter√≠sticas especiales:
1. **Reducci√≥n de dimensiones**:
   - Aplica PCA para reducir a m√°ximo 10 componentes
   - **Prop√≥sito**: Optimizar uso de memoria (Agglomerative es costoso)

2. **Fallback**:
   - Si `n_clusters` es None o inv√°lido, usa `fallback_n_clusters=3`
   - **Prop√≥sito**: Robustez ante par√°metros incorrectos

##### Par√°metros:
- `n_clusters`: 3 (por defecto, con fallback)
- `linkage`: "ward" (m√©todo de enlace jer√°rquico)

##### Outputs:
- **`modelo_agglomerative`**: Modelo AgglomerativeClustering (PickleDataset)
- **`labels_agglomerative`**: Etiquetas de cluster (PickleDataset)

#### **NODO 10: `evaluate_agglomerative_node`**
**Funci√≥n**: `evaluate_clustering()`

##### Output: `metricas_agglomerative`
- **Tipo**: `pickle.PickleDataset`
- **Ubicaci√≥n**: `data/08_reporting/metricas_agglomerative.pickle`

#### **NODO 11: `add_clusters_agglomerative_node`**
**Funci√≥n**: `add_cluster_labels_to_data()`

##### Output: `datos_con_clusters_agglomerative`
- **Tipo**: `pandas.ParquetDataset`
- **Ubicaci√≥n**: `data/05_model_input/datos_con_clusters_agglomerative.parquet`

---

### **NODOS 12-14: Gaussian Mixture Model (GMM)**

#### **NODO 12: `train_gmm_node`**
**Funci√≥n**: `train_gmm_with_fallback()`

##### ¬øQu√© hace?
- **Entrena un modelo GMM** (Gaussian Mixture Model)
- **Modela los datos** como una mezcla de distribuciones gaussianas
- **Tiene fallback** si n_components es inv√°lido

##### Caracter√≠sticas:
- **Modelo probabil√≠stico**: Asigna probabilidades de pertenencia a cada cluster
- **M√°s flexible** que K-Means (puede modelar clusters el√≠pticos)
- **Fallback**: Usa `fallback_n_clusters=3` si n_components es inv√°lido

##### Par√°metros:
- `n_components`: 3 (n√∫mero de distribuciones gaussianas)
- `random_state`: 42
- `n_init`: 10 (intentos de inicializaci√≥n)

##### Outputs:
- **`modelo_gmm`**: Modelo GaussianMixture (PickleDataset)
- **`labels_gmm`**: Etiquetas de cluster (PickleDataset)

#### **NODO 13: `evaluate_gmm_node`**
**Funci√≥n**: `evaluate_clustering()`

##### Output: `metricas_gmm`
- **Tipo**: `pickle.PickleDataset`
- **Ubicaci√≥n**: `data/08_reporting/metricas_gmm.pickle`

#### **NODO 14: `add_clusters_gmm_node`**
**Funci√≥n**: `add_cluster_labels_to_data()`

##### Output: `datos_con_clusters_gmm`
- **Tipo**: `pandas.ParquetDataset`
- **Ubicaci√≥n**: `data/05_model_input/datos_con_clusters_gmm.parquet`

---

## üìä Resumen de Algoritmos de Clustering

### Comparaci√≥n de Algoritmos:

| Algoritmo | Tipo | N√∫mero de Clusters | Ventajas | Desventajas |
|-----------|------|-------------------|----------|-------------|
| **K-Means** | Particional | Fijo (3) | R√°pido, simple, escalable | Requiere especificar k, asume clusters esf√©ricos |
| **DBSCAN** | Basado en densidad | Autom√°tico | Encuentra outliers, clusters irregulares | Sensible a par√°metros eps y min_samples |
| **Agglomerative** | Jer√°rquico | Fijo (3) | Crea dendrograma, flexible | Costoso en memoria, usa PCA para optimizar |
| **GMM** | Probabil√≠stico | Fijo (3) | Modela clusters el√≠pticos, probabilidades | M√°s lento que K-Means, m√°s par√°metros |

---

## üéØ Prop√≥sito de Cada Algoritmo en el Contexto de Ventas:

1. **K-Means**: Segmentaci√≥n b√°sica de productos/comunas por volumen de ventas
2. **DBSCAN**: Identificar outliers y grupos de comportamiento an√≥malo
3. **Agglomerative**: Entender jerarqu√≠as y relaciones entre segmentos
4. **GMM**: Modelar distribuciones complejas de ventas con probabilidades

---

## üìù Observaciones T√©cnicas:

1. **Estandarizaci√≥n cr√≠tica**: Todos los algoritmos usan datos estandarizados (importante para que todas las features tengan el mismo peso)

2. **Muestreo**: Si hay m√°s de 10,000 muestras, se muestrea para optimizar tiempo de c√≥mputo

3. **Manejo de ruido**: DBSCAN puede identificar puntos de ruido (label = -1), que se filtran en las m√©tricas

4. **Optimizaci√≥n de memoria**: Agglomerative usa PCA para reducir dimensiones antes de clustering

5. **Reproducibilidad**: Todos los algoritmos usan `random_state=42` para resultados consistentes

---

## üîç Flujo Visual del Pipeline de Clustering:

```
ventas_preprocesadas
    ‚Üì
[prepare_clustering_data] ‚Üí X_clustering
    ‚Üì
[scale_features] ‚Üí X_clustering_scaled + scaler_clustering
    ‚Üì
    ‚îú‚îÄ‚Üí [train_kmeans] ‚Üí modelo_kmeans + labels_kmeans
    ‚îÇ       ‚Üì
    ‚îÇ   [evaluate_clustering] ‚Üí metricas_kmeans
    ‚îÇ       ‚Üì
    ‚îÇ   [add_cluster_labels] ‚Üí datos_con_clusters_kmeans
    ‚îÇ
    ‚îú‚îÄ‚Üí [train_dbscan] ‚Üí modelo_dbscan + labels_dbscan
    ‚îÇ       ‚Üì
    ‚îÇ   [evaluate_clustering] ‚Üí metricas_dbscan
    ‚îÇ       ‚Üì
    ‚îÇ   [add_cluster_labels] ‚Üí datos_con_clusters_dbscan
    ‚îÇ
    ‚îú‚îÄ‚Üí [train_agglomerative] ‚Üí modelo_agglomerative + labels_agglomerative
    ‚îÇ       ‚Üì
    ‚îÇ   [evaluate_clustering] ‚Üí metricas_agglomerative
    ‚îÇ       ‚Üì
    ‚îÇ   [add_cluster_labels] ‚Üí datos_con_clusters_agglomerative
    ‚îÇ
    ‚îî‚îÄ‚Üí [train_gmm] ‚Üí modelo_gmm + labels_gmm
            ‚Üì
        [evaluate_clustering] ‚Üí metricas_gmm
            ‚Üì
        [add_cluster_labels] ‚Üí datos_con_clusters_gmm
```

---

## ‚úÖ Outputs Finales del Pipeline de Clustering:

1. **4 Modelos entrenados** (PickleDataset):
   - `modelo_kmeans.pickle`
   - `modelo_dbscan.pickle`
   - `modelo_agglomerative.pickle`
   - `modelo_gmm.pickle`

2. **4 Sets de labels** (PickleDataset):
   - `labels_kmeans.pickle`
   - `labels_dbscan.pickle`
   - `labels_agglomerative.pickle`
   - `labels_gmm.pickle`

3. **4 Sets de m√©tricas** (PickleDataset):
   - `metricas_kmeans.pickle`
   - `metricas_dbscan.pickle`
   - `metricas_agglomerative.pickle`
   - `metricas_gmm.pickle`

4. **4 Datasets con clusters agregados** (ParquetDataset):
   - `datos_con_clusters_kmeans.parquet`
   - `datos_con_clusters_dbscan.parquet`
   - `datos_con_clusters_agglomerative.parquet`
   - `datos_con_clusters_gmm.parquet`

5. **Scaler guardado** (PickleDataset):
   - `scaler_clustering.pickle` (para estandarizar nuevos datos)

